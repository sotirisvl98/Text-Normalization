{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94d105a0-671c-4094-a642-ac22eb6f9c3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a32d64-f563-455f-b203-002ef756a52b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorWithPadding\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f14f481-1336-4ad7-87a3-1a445b4f593a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_path = 'Dataset/normalization_assesment_dataset_10k.csv'\n",
    "df = pd.read_csv(df_path)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70850271-3923-4cc1-a59f-a5a2b70601fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df['raw_comp_writers_text'].isnull().sum())\n",
    "# 1\n",
    "\n",
    "# Dropping the null row\n",
    "df.dropna()\n",
    "print(df['raw_comp_writers_text'].isnull().sum())\n",
    "# 1\n",
    "\n",
    "# It must be an empty string\n",
    "df.replace('',pd.NA, inplace = True)\n",
    "df.dropna()\n",
    "print(df['raw_comp_writers_text'].isnull().sum())\n",
    "\n",
    "# Maybe strip whitespaces\n",
    "df['raw_comp_writers_text'] = df['raw_comp_writers_text'].str.strip()\n",
    "df.replace(\"\", pd.NA, inplace=True)\n",
    "df = df.dropna()\n",
    "print(df['raw_comp_writers_text'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace46582-d7e4-457b-bce3-df290dd1929e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Using Transformers (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35cad871-6fe8-4779-ae7f-38f688baa165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The model might be biased toward predicting 0 for unseen tokens if it hasn't generalized well during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b85f89f9-9348-4efc-aca0-40838787ca99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Tokenize raw text and label tokens, if they exist in the normalized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcb5fc8f-d097-4d91-8563-e324d980a01a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_and_label(row):\n",
    "    raw_text = row['raw_comp_writers_text']\n",
    "    clean_text = row['CLEAN_TEXT']\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        raw_text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized['input_ids'].squeeze(0))\n",
    "    labels = []\n",
    "    for token in tokens:\n",
    "        if token in tokenizer.all_special_tokens:\n",
    "            labels.append(-100)  # Ignore special tokens\n",
    "        else:\n",
    "            labels.append(1 if token in tokenizer.tokenize(clean_text) else 0)\n",
    "\n",
    "    return tokenized['input_ids'].squeeze(0).tolist(), tokenized['attention_mask'].squeeze(0).tolist(), labels\n",
    "\n",
    "df.loc[:, 'token_labels'] = df.apply(tokenize_and_label, axis=1) # tuple of (tokens, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f318f12-2626-4c71-a6e5-8b248ef5f09a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ed831be-446e-4f58-9ef0-d2ed2656635b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Spliting into train,val,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d3d8ed1-e887-4d82-bbe0-f92c75ea078d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=98)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.33, random_state=98)  \n",
    "\n",
    "print(len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cbefe07-0d03-49a3-9b7d-0ee8abc5830a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "becaf064-e02c-417a-ac9b-968cbdf11e73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The NormalizationDataset class is a PyTorch Dataset designed to convert rows from a DataFrame into inputs suitable for a BERT-based token classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47978958-4de3-446d-8a02-4899eac97350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class NormalizationDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        input_ids, attention_mask, labels = row['token_labels']\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "train_dataset = NormalizationDataset(train_df)\n",
    "val_dataset = NormalizationDataset(val_df)\n",
    "test_dataset = NormalizationDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2e5377-96bb-410d-a9a0-6a922e761fd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    example = train_dataset[i]\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Input IDs: {example['input_ids']}\")\n",
    "    print(f\"Attention Mask: {example['attention_mask']}\")\n",
    "    print(f\"Labels: {example['labels']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f80c3b4-1b6c-48b2-b0e3-fe2eaca694ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fine-tune BERT for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ba5e3e-6001-4ca5-b067-5061dec82d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/tmp/results',  # Save checkpoints to a temporary directory because of memory issues\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_steps=10\n",
    ")\n",
    "\n",
    "# Use Hugging Face's built-in data collator for token classification\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,  # Automatically handles padding\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3980c27b-a0b5-423b-9940-8a9b2211493a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "MEXRI EDW KALA MALLON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc71e2dc-8028-443f-8b25-c04c69f0f59c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "try:\n",
    "    shutil.copytree('/tmp/results', './results')\n",
    "    print(\"Succesfully copied tmp results to cd\")\n",
    "except Exception as e:\n",
    "    print(f\"Error copying tmp results to cd: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dce1c62-d88a-48ff-a2de-b6df6c38b223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Use the fine-tuned model to predict and reconstruct normalized text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7be1d2d-0ea6-4951-9f4e-74fbc3947105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c261aa4f-39d3-4fbc-883b-b147c1ad0855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_on_test(test_df, model, tokenizer):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "\n",
    "        precomputed = {\n",
    "            'input_ids': torch.tensor(row['token_labels'][0]).unsqueeze(0).to(device),\n",
    "            'attention_mask': torch.tensor(row['token_labels'][1]).unsqueeze(0).to(device)\n",
    "        }\n",
    "\n",
    "        outputs = model(**precomputed)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2).squeeze(0).cpu().numpy()\n",
    "\n",
    "        true_labels = torch.tensor(row['token_labels'][2]).numpy()\n",
    "        valid_indices = true_labels != -100  # Exclude ignored tokens (e.g., [PAD], [CLS], [SEP])\n",
    "\n",
    "        all_predictions.extend(predictions[valid_indices])\n",
    "        all_labels.extend(true_labels[valid_indices])\n",
    "\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n",
    "    return precision, recall, f1\n",
    "\n",
    "precision, recall, f1 = evaluate_on_test(test_df, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd5f99af-79a2-4064-90ab-bacbbad3a8c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test model on custom text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20440d04-6832-4545-8bca-cd623323910c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_custom_text(raw_text, model, tokenizer):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        raw_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2).squeeze(0).cpu().numpy()\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze(0))\n",
    "    normalized_tokens = [\n",
    "        token for token, label in zip(tokens, predictions)\n",
    "        if label == 1 and token not in tokenizer.all_special_tokens\n",
    "    ]\n",
    "\n",
    "    normalized_text = tokenizer.convert_tokens_to_string(normalized_tokens)\n",
    "\n",
    "    return normalized_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e4cb5a-ba22-4146-813c-76d0d650b463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_texts = [\n",
    "    \"Tony Grace/Rob DeBoer\",\n",
    "    \"Budde Music/Lorenz Brunner\",\n",
    "    \"Jordan Riley/Adam Argyle/Copyright Control\"\n",
    "]\n",
    "\n",
    "for i, custom_text in enumerate(custom_texts):\n",
    "    normalized_output = test_custom_text(custom_text, model, tokenizer)\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Raw Text: {custom_text}\")\n",
    "    print(f\"Normalized Text: {normalized_output}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BERT",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "env_full",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
